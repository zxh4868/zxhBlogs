## One-hot编码

One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对**N** **个状态**进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。

#### 基本原理

假设我们有一个分类变量，其取值为["红色", "绿色", "蓝色"]，我们希望将这些值进行One-hot编码。其过程如下：

1. **确定类别数**：首先，确定所有可能的类别。对于这个例子，我们有三个类别：红色、绿色和蓝色。
2. **创建二进制向量**：为每个类别创建一个二进制向量。向量的长度等于类别的总数，每个类别对应的向量中只有一个位置为1，其余位置为0。
3. **编码每个类别**：将每个类别映射到其对应的二进制向量。

| 类别 | One-hot 编码 |
| ---- | ------------ |
| 红色 | [1, 0, 0]    |
| 绿色 | [0, 1, 0]    |
| 蓝色 | [0, 0, 1]    |

如果有一个包含这些类别的数组，例如`["红色", "蓝色", "绿色"]`，经过One-hot编码后，它们会转换为：

`[[0, 0, 1], [0, 1, 0],[1,0,0]]`。

#### 优缺点

- **避免顺序关系问题**：在某些机器学习算法中，直接使用整数编码的分类变量可能会引入错误的顺序关系。例如，如果用0代表“红色”，1代表“绿色”，2代表“蓝色”，模型可能会误以为绿色比红色大，而蓝色比绿色大。One-hot编码消除了这种误解。
- **其问题在于很难做相似度计算**：它假设词和词之间相互独立，但在大部分情况下，词和词之间是相互影响的；另外在大规模语料上，One-hot的编码长度可能为几十万、几百万甚至更大，此时，One-hot编码显然不合适。

## 分词

### 英文分词

英文分词（Tokenization）是自然语言处理（NLP）中的基本步骤之一，用于将文本拆分为更小的单元（tokens），如单词、标点符号等。分词的质量直接影响后续任务（如词性标注、命名实体识别、句法分析等）的效果。

#### 基本概念

- **Token**：在NLP中，token是指文本中的一个基本单位，通常是一个单词、数字或标点符号。分词的目标就是将文本切分成一个个token。
- **Tokenization**: Tokenization是指将输入文本分解为单个token的过程。对于英文来说，这通常涉及按空格和标点符号切分文本，但实际应用中可能会更复杂。

#### 常用的分词方法

- **基于规则的分词**：基于规则的分词方法使用预定义的规则和正则表达式来识别和分割token。这些规则通常基于空格、标点符号、大小写转换等。
- **基于词典的分词**：这种方法利用预先构建的词典来识别和切分token。它依赖于词典的完整性和准确性。
- **基于统计的分词**：基于统计的方法利用语言模型（如n-gram模型）来确定token边界。常见的技术包括最大熵模型、条件随机场（CRF）等。
  - **优点**：能处理复杂情况，适应性强
  - **缺点**：需要大量训练数据，训练过程复杂且耗时

#### Huggingface的Tokenizer工具

- **word/词**：对于英文等一些语言来说，存在着天然的分隔符，比如说空格、标点符号，对词进行切分相对容易。对于中文等东亚语言来说，需要使用某种分词算法才可以。
- **char/字符**：词汇表里只有最基本的字符，字符的数量有限，那么为每个字符学习嵌入向量时，每个向量就需要容纳太多语义，学习起来非常困难。
- **subword/字词**：它介于字符和单词之间，子词单元可以是完整的单词、词干、词缀，甚至是更小的字符序列。使用子词单元可以有效地处理未登录词（即训练集中未出现过的词）和处理不同语言的混合文本。处理原则是，**保留完整的常用单词，并将未知词（生僻词）分解为更小的子词单元。**

#### 常用Tokenizer算法

- `BertTokenizer`：适用于 BERT 模型，支持 WordPiece 分词和 Byte-Pair Encoding（BPE）分词算法。它还支持对输入序列进行截断和填充，以适应模型的输入要求。
- `RobertaTokenizer`：适用于 RoBERTa 模型，与 `BertTokenizer` 类似，但使用的是更大的词表和更严格的分词规则。同时，它还支持处理 Unicode 字符和多语言文本。
- `AlbertTokenizer`：适用于 ALBERT 模型，采用 SentencePiece 分词算法，支持截断和填充等功能。
- `GPT2Tokenizer`：适用于 GPT-2 模型，采用 Byte-Pair Encoding（BPE）分词算法，支持截断和填充等功能。它还支持对特殊标记进行编码，如 `[CLS]`、`[SEP]` 和 `[MASK]`。
- `XLNetTokenizer`：适用于 XLNet 模型，支持 SentencePiece 分词算法和 Transformer-XL 模型的相对位置编码。它还支持对输入序列进行截断和填充，以适应模型的输入要求。
- `ElectraTokenizer`：适用于 ELECTRA 模型，也支持 WordPiece 分词和 Byte-Pair Encoding（BPE）分词算法，同时还支持生成嵌入式掩码（masked embeddings）

### 中文分词

中文分词是将中文文本分割成一个个词语的过程。由于中文文本通常没有空格或其他明显的词边界标记，因此中文分词是一个具有挑战性的任务。

#### 基本问题

- **分词规范**：
- **歧义切分**
- **未登录词问题**



### 参考资料

https://juejin.cn/post/7012919332023828488

https://www.cnblogs.com/zhangxuegold/p/17591222.html
